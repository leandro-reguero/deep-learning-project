{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<div align=\"right\">\n",
        "  <img src=\"https://drive.google.com/uc?export=view&id=1J8JpP65HsHXdpJvhb_sMwn3yROyU832m\" height=\"80\" width=\"200\" style=\"float: right;\">\n",
        "</div>\n",
        "<h1><b>Data Science and Machine Learning</b></h1>\n",
        "<h2><b>Clase 27</b>: Introducción aprendizaje profundo (Deep learning)</h2>\n",
        "<h3><b>Docente</b>: <a href=\"https://www.linkedin.com/in/danielablanco/\">Daniela Blanco</a>"
      ],
      "metadata": {
        "id": "Wp4cyu16yiif"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Contenido\n",
        "\n",
        "- [1. ¿Aprendizaje profundo?](#deeplearning)\n",
        "- [2. Redes neuronales](#redes)\n",
        "  - [2.1. Arquitectura](#arquitectura)\n",
        "  - [2.2. Función de activación](#activacion)\n",
        "  - [2.3. Función de pérdida](#perdida)\n",
        "  - [2.4. Optimizador](#optimizador)\n",
        "  - [2.5. Funcionamiento](#funcionamiento)\n",
        "- [3. Descenso del gradiente](#descenso)\n",
        "  - [3.1. Implementaciones](#implementaciones)\n",
        "- [4. Tipos de redes neuronales](#tipos)\n",
        "- [5. Librerías](#librerias)\n",
        "- [6. Ejemplos de uso](#ejemplos)\n",
        "- [7. Links de interés](#links)\n"
      ],
      "metadata": {
        "id": "XEmijBfl2b_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "import warnings"
      ],
      "metadata": {
        "id": "cPtXYusox8X4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "XQZiW-SCCQSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. ¿Aprendizaje profundo? <a name=\"deeplearning\"></a>"
      ],
      "metadata": {
        "id": "Pw6lT_8L3Bny"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=15CjovalU2EC2CTS4vsf03UWOjKow32S_\" height=\"180\" width=\"370\" style=\"float: center;\">"
      ],
      "metadata": {
        "id": "XlGxqok8qZon"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Es una subdisciplina del machine learning que se enfoca en el uso de redes neuronales profundas.\n",
        "\n",
        "Estas redes neuronales se denominan \"profundas\" porque tienen muchas capas de neuronas artificiales, o \"nodos\", que pueden aprender y representar patrones de datos muy complejos.\n",
        "\n",
        "A diferencia de los algoritmos tradicionales de machine learning, que a menudo requieren de un proceso de ingeniería de características manual, los modelos de deep learning pueden aprender representaciones de alto nivel directamente de los datos brutos.\n",
        "\n",
        "Las técnicas de deep learning han impulsado muchos avances en la IA en la última década, particularmente en áreas como el reconocimiento de voz, el reconocimiento de imágenes, el procesamiento del lenguaje natural o la traducción automática.\n",
        "\n",
        "A pesar de su complejidad y requerimientos de recursos, el deep learning ha demostrado ser una herramienta extremadamente poderosa para resolver problemas de IA complejos y se espera que siga impulsando muchos avances en la IA en el futuro."
      ],
      "metadata": {
        "id": "MrE2Gc3Ygk8X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Redes neuronales <a name=\"redes\"></a>"
      ],
      "metadata": {
        "id": "0EFjo2dfrz3W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una Red de Neuronas Artificiales (ANN, Artificial Neural Networks) es un modelo de aprendizaje automático inspirado en la estructura y función del cerebro humano.\n",
        "\n",
        "Para implementar una red neuronal debemos definir su arquitectura, funciones de activación y perdida y un optimizador apropiado según la tarea a realizar."
      ],
      "metadata": {
        "id": "WatAvMCC09qF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1.Arquitectura <a name=\"arquitectura\"></a>"
      ],
      "metadata": {
        "id": "fOlKHUOFB2Kp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1IyHPuf1KGfJoH-gmhgv7t2kcdvZTj9vG\" height=\"230\" width=\"380\" style=\"float: center;\">"
      ],
      "metadata": {
        "id": "OE39eNwTaB0M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consta de un gran número de unidades de procesamiento llamadas **neuronas** (o nodos) interconectadas.\n",
        "\n",
        "Cada neurona recibe una serie de entradas, las procesa y produce una salida.\n",
        "\n",
        "Las neuronas están organizadas en **capas** (layers). Una capa de **entrada** que recibe los datos, una o más capas **ocultas** que procesan los datos, y una capa de **salida** que produce la predicción o clasificación final.\n",
        "\n",
        "Las capas ocultas procesan las entradas recibidas de la capa anterior y pasa el resultado a la siguiente capa. Una red puede tener múltiples capas ocultas.\n",
        "\n",
        "La **profundidad** (depth) de una red será la cantidad de capas.\n",
        "\n",
        "Cada conexión entre neuronas tiene un **peso** (weights) asociado.\n",
        "\n",
        "Además cada neurona tiene un valor de **sesgo** (bias) que permite ajustar la salida independientemente de las entradas. Esto ayuda a la red a ajustarse mejor a los datos.\n",
        "\n",
        "Durante el entrenamiento, los pesos se ajustan para minimizar el error de predicción."
      ],
      "metadata": {
        "id": "0xILgZKzCGgR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1hQsHQobVn_BzsHTMw3KwnpeZ_tI7tm8n\" height=\"222\" width=\"546\" style=\"float: center;\">"
      ],
      "metadata": {
        "id": "_cnIl1GUdAeJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2. Función de activación <a name=\"activacion\"></a>"
      ],
      "metadata": {
        "id": "yti-U5XT_aea"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La función de activación introduce la no linealidad en el modelo.\n",
        "\n",
        "Esto permite que la red neuronal modele relaciones complejas entre las entradas y las salidas, más allá de lo que podría hacer con solo combinaciones lineales de las entradas.\n",
        "\n",
        "Ejemplos comunes de funciones de activación incluyen la función sigmoide, la función tangente hiperbólica y la unidad lineal rectificada (ReLU)."
      ],
      "metadata": {
        "id": "vLS7OWZYBCHa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1pC3jqDkmzLdM4rBbcAvcTpXDxgInhzdG\" height=\"205\" width=\"506\" style=\"float: center;\">"
      ],
      "metadata": {
        "id": "RTDJ6UJLgCF0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3. Función de pérdida (loss) <a name=\"perdida\"></a>"
      ],
      "metadata": {
        "id": "W-t_qx8ABBT7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mide la diferencia entre la salida predicha por la red y el valor real.\n",
        "\n",
        "Nos dice que tan buena es la red neuronal.\n",
        "\n",
        "Un resultado alto indica que no está aprendiendo bien y uno bajo que va mejorando.\n",
        "\n",
        "Algunas funciones de pérdida comunes son:\n",
        "\n",
        "Ejemplos:\n",
        "- binary cross entropy (clasificación binaria),\n",
        "- categorical cross entropy (clasificación multiclase),\n",
        "- MSE (regresión), etc."
      ],
      "metadata": {
        "id": "qWmbR-JeeqS5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4. Optimizador <a name=\"optimizador\"></a>"
      ],
      "metadata": {
        "id": "Dftbb-m6ee3V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Determina la forma del aprendizaje. El proceso de ajuste de los pesos de la red para minimizar la función de pérdida.\n",
        "\n",
        "Actualiza los parámetros de la red valiéndose de la iteración previa.\n",
        "\n",
        "Ejemplos:\n",
        "- Stochactic gradient descent,\n",
        "- Root Mean Square Propagation,\n",
        "- AdaGrad, etc.\n"
      ],
      "metadata": {
        "id": "oyphlr4bf4Es"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.5. Funcionamiento <a name=\"funcionamiento\"></a>"
      ],
      "metadata": {
        "id": "YixN4KeKeob5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cada neurona recibe una serie de entradas x1​ ,x2​ ,…,xn​ que pueden provenir de los datos de entrada iniciales o de las salidas de otras neuronas en capas anteriores.\n",
        "\n",
        "A cada entrada se le asigna un peso 𝑤1,𝑤2,…,𝑤𝑛.\n",
        "\n",
        "Además de las entradas ponderadas, cada neurona tiene un valor de sesgo b.\n",
        "\n",
        "La neurona calcula una suma ponderada de las entradas y el sesgo:\n",
        "\n",
        "  **𝑧 = 𝑤1𝑥1 + 𝑤2𝑥2 +…+ 𝑤𝑛𝑥𝑛 + 𝑏**\n",
        "\n",
        "La suma ponderada 𝑧 se pasa a través de una función de activación.\n",
        "\n",
        "La salida de la función de activación se convierte en la salida final de la neurona:\n",
        "\n",
        "  **𝑦 = activacion(𝑧)**\n",
        "\n",
        "La salida 𝑦 de la neurona se puede enviar como entrada a otras neuronas en capas posteriores o puede ser la salida final de la red si la neurona está en la capa de salida."
      ],
      "metadata": {
        "id": "HTOqwBkdip3j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1puaqTfK3T9XJIWvaZSJbsQP50_8BGmf3\" height=\"300\" width=\"400\" style=\"float: center;\">"
      ],
      "metadata": {
        "id": "rqNq71c9h2zM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La red mapea datos de entrada con predicciones.\n",
        "\n",
        "La función de pérdida compara las predicciones con los target reales y da un valor de pérdida. Una métrica de cuán buena es la red.\n",
        "\n",
        "Se buscará minimizar la función de perdida en cada iteración.\n",
        "\n",
        "El optimizador usa los valores de pérdida para actualizar pesos.\n",
        "\n",
        "Una **época** (epoch) es una pasada completa por todo el conjunto de datos de entrenamiento. A menudo, se necesita entrenar la red durante muchas épocas para obtener buenos resultados.\n"
      ],
      "metadata": {
        "id": "l0TG3c-LgBZv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Descenso del gradiente <a name=\"descenso\"></a>"
      ],
      "metadata": {
        "id": "ohpWyP-uKedH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El algoritmo de optimización más utilizado para entrenar redes neuronales es el descenso del gradiente para minimizar una función de coste o pérdida.\n",
        "\n",
        "El gradiente es un cálculo que nos permite saber cómo ajustar los parámetros de la red de tal forma que se minimice su desviación a la salida.\n",
        "\n",
        "**Backpropagation** es el proceso de ajuste de los pesos mediante el cálculo del error de predicción y la propagación de este error hacia atrás a través de la red.\n",
        "\n",
        "**Tasa de aprendizaj**e (Learning Rate): Es un hiperparámetro que controla cuánto se ajustan los pesos de la red en cada iteración del entrenamiento. Un valor adecuado de la tasa de aprendizaje es crucial para el rendimiento de la red.\n",
        "\n",
        "Comienza con valores iniciales aleatorios para los parámetros y luego, en cada iteración, calcula el gradiente de la función de coste con respecto a cada parámetro. El gradiente en un punto es un vector que apunta en la dirección de la mayor pendiente en ese punto, por lo que moverse en la dirección opuesta (es decir, el \"descenso de gradiente\") reduce la función de coste.\n",
        "\n",
        "Este proceso se repite hasta que el algoritmo converge a un mínimo de la función de coste, es decir, un punto donde la función de coste no puede ser reducida más moviendo los parámetros en ninguna dirección.\n",
        "\n",
        "El algoritmo cuenta con varias versiones dependiendo del número de muestras que introducimos a la red en cada iteración.\n"
      ],
      "metadata": {
        "id": "jRomtFU2AO8v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1. Implementaciones <a name=\"implementaciones\"></a>\n"
      ],
      "metadata": {
        "id": "3XjQ2C_P_9hE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Por lotes (o batch): todos los datos disponibles se introducen de una vez.\n",
        "\n",
        "  Esto supondrá problemas de estancamiento, ya que el gradiente se calculará usando siempre todas las muestras, y llegará un momento en que las variaciones serán mínimas\n",
        "\n",
        "- Estocástico: se introduce una única muestra aleatoria en cada iteración.\n",
        "\n",
        "  El gradiente se calculará para esa muestra concreta, lo que supone la introducción de la deseada aleatoriedad, dificultando así el estancamiento. Contra: lentitud, ya que necesita de muchas más iteraciones.\n",
        "\n",
        "- Descenso del gradiente (estocástico) en mini-lotes (o mini-batch): se introducen N muestras en cada iteración.\n",
        "\n",
        "  Conservando las ventajas de la segunda versión y consiguiendo además que el entrenamiento sea más rápido debido a la paralelización de las operaciones.\n"
      ],
      "metadata": {
        "id": "tMqHjwidAIpm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Tipos de redes neuronales <a name=\"tipos\"></a>"
      ],
      "metadata": {
        "id": "eYhOfwBNnFTJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Modelo                                        | Descripción                                                                                                                     | Uso típico                                                     |\n",
        "|-----------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------|\n",
        "| Totalmente Conectadas (FCNN) | Todas las neuronas de una capa conectadas a todas las neuronas de la capa siguiente.         | Clasificación, regresión                                       |\n",
        "| Convolucionales (CNN)        | Para procesar datos con una estructura de cuadrícula topológica, como una imagen.                               | Procesamiento y clasificación de imágenes                      |\n",
        "| Recurrentes (RNN)            | Para procesar secuencias de datos, teniendo en cuenta el orden de los datos.                                    | Procesamiento del lenguaje natural, series temporales          |\n",
        "| Autoencoders                                  | Modelo que aprende a copiar sus entradas a sus salidas. Se utiliza para aprender la representación de los datos.                | Reducción de la dimensionalidad, generación de nuevas imágenes |\n",
        "| Redes Generativas Adversativas (GAN)          | Sistema de dos redes neuronales que compiten entre sí: una red genera nuevos datos y la otra evalúa su autenticidad.            | Generación de nuevas imágenes, superresolución                 |\n",
        "| Transformers                                  | Modelo basado en la atención que procesa los datos de entrada en paralelo en lugar de secuencialmente, mejorando la eficiencia. | Procesamiento del lenguaje natural (por ejemplo, BERT, GPT)    |"
      ],
      "metadata": {
        "id": "8CFhE7ivnFvU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Librerías <a name=\"librerias\"></a>"
      ],
      "metadata": {
        "id": "e74Q5tmyn3uw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Los modelos de aprendizaje profundo se pueden implementar utilizando diversas bibliotecas y marcos de trabajo. Algunas de las más populares son `TensorFlow` y `Keras`.\n",
        "\n",
        "- TensorFlow: Es una biblioteca de código abierto para el aprendizaje automático desarrollada por Google. Proporciona un conjunto de herramientas para desarrollar y entrenar modelos de Machine Learning y Deep Learning.\n",
        "\n",
        "  Soporta una amplia gama de algoritmos y técnicas. Además, puede ser ejecutado en múltiples CPUs y GPUs.\n",
        "\n",
        "- Keras: Es una API de redes neuronales de alto nivel, escrita en Python y capaz de correr sobre TensorFlow, CNTK, o Theano. Fue desarrollada con el objetivo de permitir una experimentación rápida y ser amigable, modular y extensible\n",
        "\n",
        "Además de TensorFlow y Keras, existen otras bibliotecas y marcos de trabajo como PyTorch, Caffe, MXNet, etc., que también son muy populares para la implementación de modelos de deep learning."
      ],
      "metadata": {
        "id": "ZaaLHC5Vn8J9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Ejemplos de uso <a name=\"ejemplos\"></a>"
      ],
      "metadata": {
        "id": "EoZ6P8ui3jq6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejemplo 1: Pasar grados de Celsius a Fahrenheit"
      ],
      "metadata": {
        "id": "cH_XE76-lQZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# datos\n",
        "celsius = np.array([-40, -10, 0, 8, 15, 22, 38], dtype=float)\n",
        "fahrenheit = np.array([-40, 14, 32, 46, 59, 72, 100], dtype=float)"
      ],
      "metadata": {
        "id": "3-DNg7A0inkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Modelo simple**"
      ],
      "metadata": {
        "id": "e-z9no8ildxj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# una capa de entrada con una sola neurona (input_shape)\n",
        "# una capa de salida con una neurona\n",
        "# sin capas ocultas\n",
        "capa = tf.keras.layers.Dense(units=1, input_shape=[1])\n",
        "\n",
        "modelo = tf.keras.Sequential([capa])"
      ],
      "metadata": {
        "id": "CIBSbn51lgqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# configuracion del modelo\n",
        "modelo.compile(\n",
        "    optimizer = tf.keras.optimizers.Adam(0.1),\n",
        "    loss = 'mean_squared_error'\n",
        ")"
      ],
      "metadata": {
        "id": "3mmDx1oUlrHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# entrenamiento\n",
        "historial = modelo.fit(celsius, fahrenheit, epochs=1000, verbose=True)"
      ],
      "metadata": {
        "id": "wiWwil93l3qW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizamos las perdidas"
      ],
      "metadata": {
        "id": "5Iej74Nal9SO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.xlabel(\"# Epoca\")\n",
        "plt.ylabel(\"Magnitud de pérdida\")\n",
        "plt.plot(historial.history[\"loss\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "njV8u6bCl-p3",
        "outputId": "3742ab4b-e410-47ed-ac37-d5165c47223a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f7c6cb99030>]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Hacemos la predicción"
      ],
      "metadata": {
        "id": "ynSQf3XnpcKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resultado = modelo.predict([100.0])\n",
        "\n",
        "print(\"El resultado es \" + str(resultado) + \" fahrenheit!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtuxBKUJqCsr",
        "outputId": "221fadaf-515b-4632-ab09-d08d7e5738ea"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 102ms/step\n",
            "El resultado es [[211.7417]] fahrenheit!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parámetros del modelo"
      ],
      "metadata": {
        "id": "qDCbYttWnevf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(capa.get_weights())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmv0iIAGn879",
        "outputId": "84a7a464-886a-40eb-e4c5-744fc9455506"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([[1.7983065]], dtype=float32), array([31.911062], dtype=float32)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**¿Cuál era la fórmula matemática para realizar la conversión?**\n",
        "\n",
        "Fahrenheit = Celsius * 1.8 + 32"
      ],
      "metadata": {
        "id": "1rAxa3rppiCn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Modelo más complejo**"
      ],
      "metadata": {
        "id": "RvhrhNCyoBcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "oculta1 = tf.keras.layers.Dense(units=3, input_shape=[1])\n",
        "oculta2 = tf.keras.layers.Dense(units=3)\n",
        "salida = tf.keras.layers.Dense(units=1)\n",
        "\n",
        "modelo = tf.keras.Sequential([oculta1, oculta2, salida])"
      ],
      "metadata": {
        "id": "qWWIF6mvoFzM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelo.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(0.1),\n",
        "    loss='mean_squared_error'\n",
        ")"
      ],
      "metadata": {
        "id": "xgFJidCgoTZE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "historial = modelo.fit(celsius, fahrenheit, epochs=1000)"
      ],
      "metadata": {
        "id": "dS0zlffsorDo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9c4e00f-95a1-4fc3-ea0a-f91605cc3465"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "1/1 [==============================] - 1s 895ms/step - loss: 2676.7446\n",
            "Epoch 2/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1393.0251\n",
            "Epoch 3/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 898.3265\n",
            "Epoch 4/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1280.6777\n",
            "Epoch 5/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1300.4397\n",
            "Epoch 6/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 990.1596\n",
            "Epoch 7/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 776.9093\n",
            "Epoch 8/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 766.9136\n",
            "Epoch 9/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 850.1667\n",
            "Epoch 10/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 894.8336\n",
            "Epoch 11/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 851.9434\n",
            "Epoch 12/1000\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 744.0801\n",
            "Epoch 13/1000\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 622.7980\n",
            "Epoch 14/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 537.8846\n",
            "Epoch 15/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 513.6422\n",
            "Epoch 16/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 531.5453\n",
            "Epoch 17/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 541.0380\n",
            "Epoch 18/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 505.8297\n",
            "Epoch 19/1000\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 434.8062\n",
            "Epoch 20/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 363.2402\n",
            "Epoch 21/1000\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 317.8377\n",
            "Epoch 22/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 300.9450\n",
            "Epoch 23/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 296.6129\n",
            "Epoch 24/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 285.2693\n",
            "Epoch 25/1000\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 255.6625\n",
            "Epoch 26/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 209.7486\n",
            "Epoch 27/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 160.4758\n",
            "Epoch 28/1000\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 123.5870\n",
            "Epoch 29/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 106.0709\n",
            "Epoch 30/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 98.9033\n",
            "Epoch 31/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 84.6819\n",
            "Epoch 32/1000\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 56.7883\n",
            "Epoch 33/1000\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 27.4247\n",
            "Epoch 34/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 13.5772\n",
            "Epoch 35/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 17.3493\n",
            "Epoch 36/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 23.2869\n",
            "Epoch 37/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 17.6208\n",
            "Epoch 38/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 6.0011\n",
            "Epoch 39/1000\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 5.1023\n",
            "Epoch 40/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 18.0031\n",
            "Epoch 41/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 28.4146\n",
            "Epoch 42/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 26.0574\n",
            "Epoch 43/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 21.4256\n",
            "Epoch 44/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 25.4444\n",
            "Epoch 45/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 31.6447\n",
            "Epoch 46/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 29.2814\n",
            "Epoch 47/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 20.9587\n",
            "Epoch 48/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 16.1904\n",
            "Epoch 49/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 16.1935\n",
            "Epoch 50/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 14.4504\n",
            "Epoch 51/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 8.7124\n",
            "Epoch 52/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 3.6415\n",
            "Epoch 53/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.6524\n",
            "Epoch 54/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 3.6692\n",
            "Epoch 55/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 3.3097\n",
            "Epoch 56/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.4521\n",
            "Epoch 57/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.4349\n",
            "Epoch 58/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.4534\n",
            "Epoch 59/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 3.1960\n",
            "Epoch 60/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 3.8726\n",
            "Epoch 61/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 3.4288\n",
            "Epoch 62/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 3.1421\n",
            "Epoch 63/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 3.6838\n",
            "Epoch 64/1000\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.3832\n",
            "Epoch 65/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 4.3071\n",
            "Epoch 66/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 3.4669\n",
            "Epoch 67/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.6743\n",
            "Epoch 68/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.4330\n",
            "Epoch 69/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.3741\n",
            "Epoch 70/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.9117\n",
            "Epoch 71/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.1157\n",
            "Epoch 72/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.5675\n",
            "Epoch 73/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5119\n",
            "Epoch 74/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5806\n",
            "Epoch 75/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.4047\n",
            "Epoch 76/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.1355\n",
            "Epoch 77/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.1383\n",
            "Epoch 78/1000\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.4005\n",
            "Epoch 79/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5742\n",
            "Epoch 80/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5245\n",
            "Epoch 81/1000\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.4774\n",
            "Epoch 82/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.5916\n",
            "Epoch 83/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.7098\n",
            "Epoch 84/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.6510\n",
            "Epoch 85/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.4972\n",
            "Epoch 86/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.4270\n",
            "Epoch 87/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.4296\n",
            "Epoch 88/1000\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.3679\n",
            "Epoch 89/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.2309\n",
            "Epoch 90/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.1395\n",
            "Epoch 91/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.1384\n",
            "Epoch 92/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.1450\n",
            "Epoch 93/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.1041\n",
            "Epoch 94/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0681\n",
            "Epoch 95/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0906\n",
            "Epoch 96/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.1360\n",
            "Epoch 97/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.1464\n",
            "Epoch 98/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.1326\n",
            "Epoch 99/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.1396\n",
            "Epoch 100/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.1648\n",
            "Epoch 101/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.1691\n",
            "Epoch 102/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.1457\n",
            "Epoch 103/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.1268\n",
            "Epoch 104/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.1255\n",
            "Epoch 105/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.1203\n",
            "Epoch 106/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0985\n",
            "Epoch 107/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0787\n",
            "Epoch 108/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0757\n",
            "Epoch 109/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0777\n",
            "Epoch 110/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0711\n",
            "Epoch 111/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0639\n",
            "Epoch 112/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0677\n",
            "Epoch 113/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0760\n",
            "Epoch 114/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0774\n",
            "Epoch 115/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0750\n",
            "Epoch 116/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0774\n",
            "Epoch 117/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0820\n",
            "Epoch 118/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0810\n",
            "Epoch 119/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0763\n",
            "Epoch 120/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0743\n",
            "Epoch 121/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0744\n",
            "Epoch 122/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0717\n",
            "Epoch 123/1000\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.0672\n",
            "Epoch 124/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0653\n",
            "Epoch 125/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0658\n",
            "Epoch 126/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0651\n",
            "Epoch 127/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0634\n",
            "Epoch 128/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0636\n",
            "Epoch 129/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0652\n",
            "Epoch 130/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0658\n",
            "Epoch 131/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0653\n",
            "Epoch 132/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0657\n",
            "Epoch 133/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0666\n",
            "Epoch 134/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0665\n",
            "Epoch 135/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0656\n",
            "Epoch 136/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0652\n",
            "Epoch 137/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0652\n",
            "Epoch 138/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0646\n",
            "Epoch 139/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0638\n",
            "Epoch 140/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0635\n",
            "Epoch 141/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0636\n",
            "Epoch 142/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0634\n",
            "Epoch 143/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0631\n",
            "Epoch 144/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0632\n",
            "Epoch 145/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0636\n",
            "Epoch 146/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0636\n",
            "Epoch 147/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0635\n",
            "Epoch 148/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0637\n",
            "Epoch 149/1000\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0638\n",
            "Epoch 150/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0636\n",
            "Epoch 151/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0635\n",
            "Epoch 152/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0635\n",
            "Epoch 153/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0634\n",
            "Epoch 154/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0632\n",
            "Epoch 155/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0631\n",
            "Epoch 156/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0632\n",
            "Epoch 157/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0631\n",
            "Epoch 158/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0631\n",
            "Epoch 159/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0631\n",
            "Epoch 160/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0631\n",
            "Epoch 161/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0632\n",
            "Epoch 162/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0631\n",
            "Epoch 163/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0632\n",
            "Epoch 164/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0632\n",
            "Epoch 165/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0632\n",
            "Epoch 166/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0631\n",
            "Epoch 167/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0631\n",
            "Epoch 168/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0631\n",
            "Epoch 169/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0631\n",
            "Epoch 170/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0631\n",
            "Epoch 171/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0631\n",
            "Epoch 172/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0631\n",
            "Epoch 173/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0630\n",
            "Epoch 174/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0631\n",
            "Epoch 175/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0631\n",
            "Epoch 176/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0631\n",
            "Epoch 177/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0631\n",
            "Epoch 178/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0631\n",
            "Epoch 179/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0631\n",
            "Epoch 180/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0631\n",
            "Epoch 181/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0631\n",
            "Epoch 182/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0631\n",
            "Epoch 183/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0631\n",
            "Epoch 184/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0631\n",
            "Epoch 185/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0631\n",
            "Epoch 186/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 187/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0630\n",
            "Epoch 188/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 189/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0631\n",
            "Epoch 190/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0631\n",
            "Epoch 191/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 192/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0631\n",
            "Epoch 193/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0631\n",
            "Epoch 194/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0631\n",
            "Epoch 195/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0631\n",
            "Epoch 196/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 197/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 198/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 199/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 200/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0630\n",
            "Epoch 201/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 202/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 203/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 204/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 205/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 206/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 207/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 208/1000\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0630\n",
            "Epoch 209/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 210/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 211/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0630\n",
            "Epoch 212/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 213/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 214/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 215/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 216/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 217/1000\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0630\n",
            "Epoch 218/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 219/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 220/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 221/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 222/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 223/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 224/1000\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0630\n",
            "Epoch 225/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 226/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 227/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 228/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 229/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 230/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 231/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 232/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 233/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 234/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 235/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 236/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0630\n",
            "Epoch 237/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 238/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 239/1000\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0630\n",
            "Epoch 240/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 241/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 242/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 243/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 244/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 245/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 246/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 247/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0630\n",
            "Epoch 248/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0630\n",
            "Epoch 249/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0630\n",
            "Epoch 250/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 251/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 252/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 253/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 254/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 255/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 256/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 257/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 258/1000\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0630\n",
            "Epoch 259/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 260/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 261/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 262/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 263/1000\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.0630\n",
            "Epoch 264/1000\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.0630\n",
            "Epoch 265/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 266/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0630\n",
            "Epoch 267/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 268/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 269/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0630\n",
            "Epoch 270/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 271/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 272/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 273/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 274/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 275/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 276/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 277/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 278/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0630\n",
            "Epoch 279/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 280/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 281/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 282/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 283/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 284/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 285/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 286/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 287/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 288/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 289/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 290/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 291/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0630\n",
            "Epoch 292/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 293/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 294/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 295/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 296/1000\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0630\n",
            "Epoch 297/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 298/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 299/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 300/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0630\n",
            "Epoch 301/1000\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0630\n",
            "Epoch 302/1000\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.0630\n",
            "Epoch 303/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0630\n",
            "Epoch 304/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 305/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 306/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 307/1000\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.0630\n",
            "Epoch 308/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0630\n",
            "Epoch 309/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 310/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 311/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 312/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 313/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 314/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 315/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 316/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 317/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 318/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0630\n",
            "Epoch 319/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 320/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 321/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 322/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 323/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 324/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0630\n",
            "Epoch 325/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 326/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 327/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 328/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 329/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 330/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 331/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 332/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 333/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 334/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 335/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0630\n",
            "Epoch 336/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 337/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 338/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0630\n",
            "Epoch 339/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 340/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0630\n",
            "Epoch 341/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 342/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 343/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 344/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0630\n",
            "Epoch 345/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 346/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 347/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 348/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 349/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 350/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0630\n",
            "Epoch 351/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 352/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 353/1000\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0630\n",
            "Epoch 354/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0630\n",
            "Epoch 355/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 356/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 357/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 358/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 359/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 360/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0630\n",
            "Epoch 361/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 362/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0630\n",
            "Epoch 363/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0630\n",
            "Epoch 364/1000\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.0630\n",
            "Epoch 365/1000\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.0630\n",
            "Epoch 366/1000\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.0630\n",
            "Epoch 367/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0630\n",
            "Epoch 368/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0630\n",
            "Epoch 369/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0630\n",
            "Epoch 370/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 371/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0630\n",
            "Epoch 372/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 373/1000\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.0630\n",
            "Epoch 374/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 375/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0630\n",
            "Epoch 376/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 377/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 378/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 379/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 380/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 381/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 382/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 383/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 384/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 385/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 386/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 387/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 388/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 389/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 390/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 391/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 392/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 393/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 394/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 395/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 396/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 397/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 398/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 399/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 400/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 401/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 402/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 403/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 404/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 405/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 406/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 407/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0630\n",
            "Epoch 408/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 409/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 410/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 411/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0630\n",
            "Epoch 412/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 413/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 414/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 415/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 416/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0630\n",
            "Epoch 417/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 418/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0630\n",
            "Epoch 419/1000\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0630\n",
            "Epoch 420/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 421/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 422/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 423/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0630\n",
            "Epoch 424/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 425/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 426/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 427/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 428/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 429/1000\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.0630\n",
            "Epoch 430/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0630\n",
            "Epoch 431/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 432/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 433/1000\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.0630\n",
            "Epoch 434/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 435/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 436/1000\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.0630\n",
            "Epoch 437/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 438/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 439/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 440/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 441/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 442/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0630\n",
            "Epoch 443/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0630\n",
            "Epoch 444/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 445/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 446/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 447/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0630\n",
            "Epoch 448/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 449/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 450/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 451/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 452/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 453/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0630\n",
            "Epoch 454/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 455/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 456/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 457/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 458/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 459/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 460/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0630\n",
            "Epoch 461/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0630\n",
            "Epoch 462/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 463/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 464/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0630\n",
            "Epoch 465/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 466/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 467/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0630\n",
            "Epoch 468/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 469/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 470/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 471/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 472/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 473/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 474/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 475/1000\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0630\n",
            "Epoch 476/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0630\n",
            "Epoch 477/1000\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.0630\n",
            "Epoch 478/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0630\n",
            "Epoch 479/1000\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0630\n",
            "Epoch 480/1000\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0630\n",
            "Epoch 481/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0630\n",
            "Epoch 482/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 483/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 484/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 485/1000\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.0630\n",
            "Epoch 486/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 487/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 488/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 489/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 490/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 491/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 492/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 493/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 494/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 495/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 496/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 497/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 498/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 499/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 500/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 501/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 502/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 503/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 504/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 505/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 506/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 507/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 508/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 509/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 510/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 511/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 512/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 513/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 514/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 515/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 516/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 517/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 518/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 519/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 520/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 521/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 522/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 523/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 524/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 525/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 526/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 527/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 528/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 529/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 530/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 531/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 532/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 533/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 534/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 535/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 536/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 537/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 538/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 539/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 540/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 541/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 542/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 543/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 544/1000\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0630\n",
            "Epoch 545/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 546/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 547/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 548/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 549/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 550/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 551/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 552/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 553/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 554/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 555/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 556/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 557/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 558/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 559/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 560/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0630\n",
            "Epoch 561/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 562/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 563/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 564/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 565/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 566/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 567/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 568/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 569/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 570/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 571/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 572/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 573/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 574/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 575/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 576/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 577/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 578/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 579/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 580/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 581/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 582/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 583/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0630\n",
            "Epoch 584/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 585/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 586/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 587/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 588/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 589/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 590/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 591/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 592/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 593/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 594/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 595/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 596/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 597/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 598/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 599/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 600/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 601/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 602/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 603/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 604/1000\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0630\n",
            "Epoch 605/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0630\n",
            "Epoch 606/1000\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.0630\n",
            "Epoch 607/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0630\n",
            "Epoch 608/1000\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0630\n",
            "Epoch 609/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0630\n",
            "Epoch 610/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 611/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0630\n",
            "Epoch 612/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0630\n",
            "Epoch 613/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 614/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0630\n",
            "Epoch 615/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 616/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 617/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 618/1000\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.0630\n",
            "Epoch 619/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 620/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 621/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 622/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 623/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 624/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 625/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0630\n",
            "Epoch 626/1000\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0630\n",
            "Epoch 627/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0630\n",
            "Epoch 628/1000\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.0630\n",
            "Epoch 629/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 630/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 631/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 632/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 633/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 634/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 635/1000\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0630\n",
            "Epoch 636/1000\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.0630\n",
            "Epoch 637/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 638/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 639/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 640/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 641/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 642/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 643/1000\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0630\n",
            "Epoch 644/1000\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0630\n",
            "Epoch 645/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0630\n",
            "Epoch 646/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 647/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 648/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 649/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 650/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 651/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0630\n",
            "Epoch 652/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 653/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 654/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 655/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 656/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 657/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 658/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 659/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 660/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0630\n",
            "Epoch 661/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 662/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 663/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 664/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 665/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 666/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 667/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0630\n",
            "Epoch 668/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 669/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 670/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 671/1000\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.0630\n",
            "Epoch 672/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 673/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 674/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 675/1000\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0630\n",
            "Epoch 676/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 677/1000\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0630\n",
            "Epoch 678/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 679/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 680/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0630\n",
            "Epoch 681/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 682/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0630\n",
            "Epoch 683/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 684/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0630\n",
            "Epoch 685/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 686/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0630\n",
            "Epoch 687/1000\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0630\n",
            "Epoch 688/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0630\n",
            "Epoch 689/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 690/1000\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0630\n",
            "Epoch 691/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 692/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 693/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 694/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 695/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0630\n",
            "Epoch 696/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 697/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 698/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 699/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0630\n",
            "Epoch 700/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 701/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 702/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0630\n",
            "Epoch 703/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 704/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 705/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 706/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 707/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 708/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 709/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 710/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 711/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 712/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 713/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 714/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 715/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 716/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 717/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 718/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 719/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 720/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 721/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 722/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 723/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 724/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 725/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 726/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 727/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 728/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 729/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 730/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 731/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 732/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 733/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 734/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 735/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 736/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 737/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 738/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 739/1000\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0630\n",
            "Epoch 740/1000\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0630\n",
            "Epoch 741/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 742/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 743/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 744/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 745/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 746/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 747/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 748/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 749/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 750/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 751/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 752/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 753/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 754/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 755/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 756/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 757/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 758/1000\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0630\n",
            "Epoch 759/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 760/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 761/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 762/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 763/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 764/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 765/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 766/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 767/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 768/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 769/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 770/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 771/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 772/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 773/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 774/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 775/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 776/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0630\n",
            "Epoch 777/1000\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.0630\n",
            "Epoch 778/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 779/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 780/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 781/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 782/1000\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.0630\n",
            "Epoch 783/1000\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.0630\n",
            "Epoch 784/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0630\n",
            "Epoch 785/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0630\n",
            "Epoch 786/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 787/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 788/1000\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0630\n",
            "Epoch 789/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 790/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0630\n",
            "Epoch 791/1000\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0630\n",
            "Epoch 792/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0630\n",
            "Epoch 793/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 794/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 795/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 796/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 797/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 798/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 799/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 800/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 801/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 802/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 803/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 804/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 805/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 806/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 807/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 808/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 809/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 810/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0630\n",
            "Epoch 811/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 812/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0630\n",
            "Epoch 813/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0630\n",
            "Epoch 814/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 815/1000\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.0630\n",
            "Epoch 816/1000\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.0630\n",
            "Epoch 817/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0630\n",
            "Epoch 818/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0630\n",
            "Epoch 819/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 820/1000\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0630\n",
            "Epoch 821/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 822/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 823/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 824/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 825/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0630\n",
            "Epoch 826/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0630\n",
            "Epoch 827/1000\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0630\n",
            "Epoch 828/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0630\n",
            "Epoch 829/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 830/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 831/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 832/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0630\n",
            "Epoch 833/1000\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0630\n",
            "Epoch 834/1000\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0630\n",
            "Epoch 835/1000\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.0630\n",
            "Epoch 836/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0630\n",
            "Epoch 837/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0630\n",
            "Epoch 838/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 839/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0630\n",
            "Epoch 840/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 841/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 842/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 843/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 844/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 845/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 846/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 847/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 848/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 849/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 850/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 851/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 852/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 853/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 854/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 855/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 856/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 857/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 858/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 859/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 860/1000\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0630\n",
            "Epoch 861/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 862/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 863/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 864/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 865/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 866/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 867/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 868/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 869/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 870/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 871/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 872/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 873/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 874/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 875/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 876/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 877/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 878/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 879/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 880/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 881/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 882/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 883/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 884/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 885/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 886/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 887/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 888/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 889/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 890/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 891/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 892/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 893/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 894/1000\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.0630\n",
            "Epoch 895/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0630\n",
            "Epoch 896/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 897/1000\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.0630\n",
            "Epoch 898/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0630\n",
            "Epoch 899/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0630\n",
            "Epoch 900/1000\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.0630\n",
            "Epoch 901/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0630\n",
            "Epoch 902/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0630\n",
            "Epoch 903/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0630\n",
            "Epoch 904/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 905/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 906/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 907/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 908/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0630\n",
            "Epoch 909/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 910/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 911/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 912/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 913/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 914/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 915/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 916/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 917/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 918/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 919/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 920/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 921/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 922/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 923/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 924/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 925/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 926/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 927/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 928/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 929/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 930/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 931/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 932/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 933/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 934/1000\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0630\n",
            "Epoch 935/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 936/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 937/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 938/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 939/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 940/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 941/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 942/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 943/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 944/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 945/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 946/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 947/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 948/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 949/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 950/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 951/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 952/1000\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0630\n",
            "Epoch 953/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 954/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0630\n",
            "Epoch 955/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0630\n",
            "Epoch 956/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0630\n",
            "Epoch 957/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 958/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 959/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 960/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 961/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 962/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 963/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 964/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 965/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 966/1000\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0630\n",
            "Epoch 967/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 968/1000\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0630\n",
            "Epoch 969/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 970/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 971/1000\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0630\n",
            "Epoch 972/1000\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0630\n",
            "Epoch 973/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0630\n",
            "Epoch 974/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 975/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 976/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 977/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 978/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 979/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 980/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0630\n",
            "Epoch 981/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0630\n",
            "Epoch 982/1000\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0630\n",
            "Epoch 983/1000\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0630\n",
            "Epoch 984/1000\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0630\n",
            "Epoch 985/1000\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.0630\n",
            "Epoch 986/1000\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.0630\n",
            "Epoch 987/1000\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.0630\n",
            "Epoch 988/1000\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0630\n",
            "Epoch 989/1000\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0630\n",
            "Epoch 990/1000\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0630\n",
            "Epoch 991/1000\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.0630\n",
            "Epoch 992/1000\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0630\n",
            "Epoch 993/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 994/1000\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.0630\n",
            "Epoch 995/1000\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0630\n",
            "Epoch 996/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 997/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 998/1000\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0630\n",
            "Epoch 999/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n",
            "Epoch 1000/1000\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0630\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.xlabel(\"# Epoca\")\n",
        "plt.ylabel(\"Magnitud de pérdida\")\n",
        "plt.plot(historial.history[\"loss\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "fNJo9-RSoukG",
        "outputId": "76503036-1a5c-42f9-bdf0-9c846781b30e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f7c7ca43100>]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resultado = modelo.predict([100.0])\n",
        "\n",
        "print(\"El resultado es \" + str(resultado) + \" fahrenheit!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJXRPi60xJzf",
        "outputId": "74230b92-b40c-44e8-efc9-f9ae50342e55"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 79ms/step\n",
            "El resultado es [[211.74742]] fahrenheit!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Variables internas del modelo\")\n",
        "\n",
        "print(oculta1.get_weights())\n",
        "print(oculta2.get_weights())\n",
        "print(salida.get_weights())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WzcdiaqzL2M",
        "outputId": "1f988d21-b0ab-4bec-d60f-ac82d23901ca"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variables internas del modelo\n",
            "[array([[ 0.73367506,  0.03891382, -0.15171172]], dtype=float32), array([ 3.5723503 , -3.3380754 ,  0.29076993], dtype=float32)]\n",
            "[array([[ 0.3894577 , -1.2030257 , -0.9263961 ],\n",
            "       [ 0.3806248 ,  0.881087  ,  0.9511756 ],\n",
            "       [ 0.34079993,  0.53006446, -1.1308819 ]], dtype=float32), array([-2.9749277, -3.398304 , -3.4059384], dtype=float32)]\n",
            "[array([[-0.61096543],\n",
            "       [-1.5895686 ],\n",
            "       [-1.0054418 ]], dtype=float32), array([3.3310134], dtype=float32)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejemplo 2: Diabetes"
      ],
      "metadata": {
        "id": "wJu2uwxKrK-o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a utilizar el conjunto de datos de inicio de diabetes de los indios Pima.\n",
        "\n",
        "Describe los datos de los registros médicos de los pacientes de los indios Pima y si tuvieron un inicio de diabetes dentro de los cinco años."
      ],
      "metadata": {
        "id": "4wsq4Q-C7fiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# conexion drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "8q9z0ZpFr2Hm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e63339ec-8b43-4b25-fc1b-50ad6e2e99b9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "archivo = '/content/drive/MyDrive/4Geeks/cursadas/ds_pt_8/data/clean-pima-indians-diabetes.csv'\n",
        "total_data  = pd.read_csv(archivo)"
      ],
      "metadata": {
        "id": "7iIuE5r78C7O"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = total_data.drop(\"8\", axis = 1)\n",
        "y = total_data[\"8\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
        "\n",
        "X_train.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "e2zI3xjh8JsX",
        "outputId": "1c781a78-e6b9-4b3b-c230-cc608af330b0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            0         1         2         3         4         5         6  \\\n",
              "60  -0.547919 -1.154694 -3.572597 -1.288212 -0.692891 -4.060474 -0.507006   \n",
              "618  1.530847 -0.278373  0.666618  0.217261 -0.692891 -0.481351  2.446670   \n",
              "346 -0.844885  0.566649 -1.194501 -0.096379  0.027790 -0.417892  0.550035   \n",
              "294 -1.141852  1.255187 -0.987710 -1.288212 -0.692891 -1.280942 -0.658012   \n",
              "231  0.639947  0.410164  0.563223  1.032726  2.519781  1.803195 -0.706334   \n",
              "\n",
              "            7  \n",
              "60  -1.041549  \n",
              "618  1.425995  \n",
              "346 -0.956462  \n",
              "294  2.702312  \n",
              "231  1.085644  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-07bc777a-ecab-4c53-bae8-b7a66da310da\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>-0.547919</td>\n",
              "      <td>-1.154694</td>\n",
              "      <td>-3.572597</td>\n",
              "      <td>-1.288212</td>\n",
              "      <td>-0.692891</td>\n",
              "      <td>-4.060474</td>\n",
              "      <td>-0.507006</td>\n",
              "      <td>-1.041549</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>618</th>\n",
              "      <td>1.530847</td>\n",
              "      <td>-0.278373</td>\n",
              "      <td>0.666618</td>\n",
              "      <td>0.217261</td>\n",
              "      <td>-0.692891</td>\n",
              "      <td>-0.481351</td>\n",
              "      <td>2.446670</td>\n",
              "      <td>1.425995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>346</th>\n",
              "      <td>-0.844885</td>\n",
              "      <td>0.566649</td>\n",
              "      <td>-1.194501</td>\n",
              "      <td>-0.096379</td>\n",
              "      <td>0.027790</td>\n",
              "      <td>-0.417892</td>\n",
              "      <td>0.550035</td>\n",
              "      <td>-0.956462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>294</th>\n",
              "      <td>-1.141852</td>\n",
              "      <td>1.255187</td>\n",
              "      <td>-0.987710</td>\n",
              "      <td>-1.288212</td>\n",
              "      <td>-0.692891</td>\n",
              "      <td>-1.280942</td>\n",
              "      <td>-0.658012</td>\n",
              "      <td>2.702312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>231</th>\n",
              "      <td>0.639947</td>\n",
              "      <td>0.410164</td>\n",
              "      <td>0.563223</td>\n",
              "      <td>1.032726</td>\n",
              "      <td>2.519781</td>\n",
              "      <td>1.803195</td>\n",
              "      <td>-0.706334</td>\n",
              "      <td>1.085644</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-07bc777a-ecab-4c53-bae8-b7a66da310da')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-07bc777a-ecab-4c53-bae8-b7a66da310da button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-07bc777a-ecab-4c53-bae8-b7a66da310da');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-cccd68e4-7d05-4bfa-bd49-09dc67dfafbe\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cccd68e4-7d05-4bfa-bd49-09dc67dfafbe')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-cccd68e4-7d05-4bfa-bd49-09dc67dfafbe button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "X_train",
              "summary": "{\n  \"name\": \"X_train\",\n  \"rows\": 614,\n  \"fields\": [\n    {\n      \"column\": \"0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.983928339924331,\n        \"min\": -1.1418515161634994,\n        \"max\": 3.906578350084603,\n        \"num_unique_values\": 17,\n        \"samples\": [\n          -0.5479185907225461,\n          1.5308466483207903,\n          0.0460143347184071\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0026058263234616,\n        \"min\": -3.78365371377963,\n        \"max\": 2.444478206307916,\n        \"num_unique_values\": 133,\n        \"samples\": [\n          -0.9982077796701244,\n          -0.9669106343430512,\n          -1.874527848828171\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.9570596922227709,\n        \"min\": -3.572597239872642,\n        \"max\": 2.734528247420465,\n        \"num_unique_values\": 44,\n        \"samples\": [\n          -1.0911052448720753,\n          -0.7809187454970045,\n          -0.5741277459136239\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.9681432943275635,\n        \"min\": -1.2882122129452358,\n        \"max\": 2.6636556358464394,\n        \"num_unique_values\": 47,\n        \"samples\": [\n          0.0290770699853224,\n          -0.410019357658197,\n          1.3463663529158807\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"4\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0092549778572082,\n        \"min\": -0.6928905722954675,\n        \"max\": 6.65283937836846,\n        \"num_unique_values\": 167,\n        \"samples\": [\n          1.2173465307140643,\n          0.9829083407992584,\n          -0.2674286720797081\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"5\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.9824342934832844,\n        \"min\": -4.060473872668307,\n        \"max\": 4.455807490825071,\n        \"num_unique_values\": 225,\n        \"samples\": [\n          -1.6363162863385974,\n          -0.0117499457616192,\n          0.2547804694892914\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"6\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.017317156703669,\n        \"min\": -1.1895531764897842,\n        \"max\": 5.88356476587794,\n        \"num_unique_values\": 443,\n        \"samples\": [\n          -0.2049944876717318,\n          -0.3922418456678032,\n          -0.6398915772109943\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"7\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.9788015652111306,\n        \"min\": -1.0415494364835025,\n        \"max\": 4.063715751598595,\n        \"num_unique_values\": 52,\n        \"samples\": [\n          0.7452933793452318,\n          1.0005566387493363,\n          1.6812586638269496\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNErcKm08NAW",
        "outputId": "03f081ae-0dcd-481d-acfc-8dc04539a28e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      1\n",
              "1      0\n",
              "2      1\n",
              "3      0\n",
              "4      1\n",
              "      ..\n",
              "763    0\n",
              "764    0\n",
              "765    0\n",
              "766    1\n",
              "767    0\n",
              "Name: 8, Length: 768, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Los modelos en Keras se definen como una secuencia de capas. Creamos un modelo secuencial y agregamos capas una a una hasta que estemos satisfechos con nuestra arquitectura de red.\n",
        "\n",
        "La capa de entrada siempre tendrá tantas neuronas como variables predictoras. En este caso, tenemos un total de 8 (de la 0 a la 7).\n",
        "\n",
        "A continuación, añadimos dos capas ocultas, una de 12 neuronas y otra de 8.\n",
        "\n",
        "Por último, la cuarta capa, de salida, tendrá una única neurona, ya que el problema es dicotómico. Si fuese de n clases, la red tendría n salidas."
      ],
      "metadata": {
        "id": "CT-CJuRi8XcR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Dense(12, input_shape = (8,), activation = \"relu\"))\n",
        "model.add(tf.keras.layers.Dense(8, activation = \"relu\"))\n",
        "model.add(tf.keras.layers.Dense(1, activation = \"sigmoid\"))"
      ],
      "metadata": {
        "id": "baXMSJs58fkc"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Al compilar, debemos especificar algunas propiedades adicionales requeridas al entrenar la red."
      ],
      "metadata": {
        "id": "Gy3DAbit88Tp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYBhjFEp84wD",
        "outputId": "9ae26fdf-ff6a-42db-f48f-d52f9cabfe41"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.engine.sequential.Sequential at 0x7f7c6aa4edd0>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_W1nqLXJMi9",
        "outputId": "98a71d87-0ee2-406d-ad59-8d50b8a4cb7b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(614, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definiremos el optimizador conocido como adam. Esta es una versión popular del descenso de gradiente porque se sintoniza automáticamente y brinda buenos resultados en una amplia gama de problemas. Recopilaremos e informaremos la precisión de la clasificación, definida a través del argumento de las métricas.\n",
        "\n",
        "El entrenamiento ocurre en épocas (epoch) y cada época se divide en lotes (batch).\n",
        "\n",
        "Epoch: Una pasada por todas las filas del conjunto de datos de entrenamiento.\n",
        "Batch: Una o más muestras consideradas por el modelo dentro de una época antes de que se actualicen los pesos.\n",
        "\n",
        "El proceso de entrenamiento se ejecutará durante un número fijo de iteraciones, que son las épocas. También debemos establecer la cantidad de filas del conjunto de datos que se consideran antes de que se actualicen los pesos del modelo dentro de cada época, lo que se denomina tamaño de batch y se establece mediante el argumento batch_size (tamaño_lote).\n",
        "\n",
        "Para este problema, ejecutaremos una pequeña cantidad de epochs (150) y usaremos un tamaño de batch relativamente pequeño de 10:"
      ],
      "metadata": {
        "id": "uT3wipJS9Lwx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train, epochs = 150, batch_size = 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2zkRCrz9XYL",
        "outputId": "842637c3-65e9-4930-87d0-9088781ee8c5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "62/62 [==============================] - 1s 2ms/step - loss: 0.7290 - accuracy: 0.5049\n",
            "Epoch 2/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6669 - accuracy: 0.6270\n",
            "Epoch 3/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.6087 - accuracy: 0.6808\n",
            "Epoch 4/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5568 - accuracy: 0.7182\n",
            "Epoch 5/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5228 - accuracy: 0.7296\n",
            "Epoch 6/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.5015 - accuracy: 0.7410\n",
            "Epoch 7/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4892 - accuracy: 0.7508\n",
            "Epoch 8/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4791 - accuracy: 0.7687\n",
            "Epoch 9/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4705 - accuracy: 0.7769\n",
            "Epoch 10/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4638 - accuracy: 0.7834\n",
            "Epoch 11/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4586 - accuracy: 0.7932\n",
            "Epoch 12/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4528 - accuracy: 0.7997\n",
            "Epoch 13/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4489 - accuracy: 0.7964\n",
            "Epoch 14/150\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 0.4455 - accuracy: 0.7997\n",
            "Epoch 15/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4418 - accuracy: 0.7899\n",
            "Epoch 16/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4391 - accuracy: 0.7948\n",
            "Epoch 17/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4376 - accuracy: 0.7915\n",
            "Epoch 18/150\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 0.4357 - accuracy: 0.7948\n",
            "Epoch 19/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4339 - accuracy: 0.7883\n",
            "Epoch 20/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4314 - accuracy: 0.7980\n",
            "Epoch 21/150\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 0.4306 - accuracy: 0.7883\n",
            "Epoch 22/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4293 - accuracy: 0.7915\n",
            "Epoch 23/150\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 0.4286 - accuracy: 0.7883\n",
            "Epoch 24/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4265 - accuracy: 0.7948\n",
            "Epoch 25/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4258 - accuracy: 0.7915\n",
            "Epoch 26/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4241 - accuracy: 0.7899\n",
            "Epoch 27/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4226 - accuracy: 0.7915\n",
            "Epoch 28/150\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 0.4220 - accuracy: 0.7932\n",
            "Epoch 29/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4211 - accuracy: 0.7964\n",
            "Epoch 30/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4203 - accuracy: 0.7948\n",
            "Epoch 31/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4194 - accuracy: 0.7932\n",
            "Epoch 32/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4176 - accuracy: 0.7997\n",
            "Epoch 33/150\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 0.4178 - accuracy: 0.7964\n",
            "Epoch 34/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4162 - accuracy: 0.7980\n",
            "Epoch 35/150\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 0.4153 - accuracy: 0.7980\n",
            "Epoch 36/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4135 - accuracy: 0.8013\n",
            "Epoch 37/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4131 - accuracy: 0.7980\n",
            "Epoch 38/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4122 - accuracy: 0.7997\n",
            "Epoch 39/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4107 - accuracy: 0.7980\n",
            "Epoch 40/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4103 - accuracy: 0.7915\n",
            "Epoch 41/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4091 - accuracy: 0.7997\n",
            "Epoch 42/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4094 - accuracy: 0.7997\n",
            "Epoch 43/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4077 - accuracy: 0.7980\n",
            "Epoch 44/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4069 - accuracy: 0.7980\n",
            "Epoch 45/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4066 - accuracy: 0.8029\n",
            "Epoch 46/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4056 - accuracy: 0.8013\n",
            "Epoch 47/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4042 - accuracy: 0.8029\n",
            "Epoch 48/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4042 - accuracy: 0.7997\n",
            "Epoch 49/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4025 - accuracy: 0.8078\n",
            "Epoch 50/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4016 - accuracy: 0.8094\n",
            "Epoch 51/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4008 - accuracy: 0.8029\n",
            "Epoch 52/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.4005 - accuracy: 0.8013\n",
            "Epoch 53/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.4007 - accuracy: 0.8013\n",
            "Epoch 54/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3987 - accuracy: 0.8046\n",
            "Epoch 55/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3982 - accuracy: 0.8094\n",
            "Epoch 56/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3969 - accuracy: 0.8111\n",
            "Epoch 57/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3960 - accuracy: 0.8078\n",
            "Epoch 58/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3953 - accuracy: 0.8078\n",
            "Epoch 59/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3947 - accuracy: 0.8062\n",
            "Epoch 60/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3940 - accuracy: 0.8078\n",
            "Epoch 61/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3929 - accuracy: 0.8127\n",
            "Epoch 62/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3923 - accuracy: 0.8111\n",
            "Epoch 63/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3920 - accuracy: 0.8094\n",
            "Epoch 64/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3915 - accuracy: 0.8111\n",
            "Epoch 65/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3906 - accuracy: 0.8094\n",
            "Epoch 66/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3898 - accuracy: 0.8078\n",
            "Epoch 67/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3897 - accuracy: 0.8078\n",
            "Epoch 68/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3892 - accuracy: 0.8094\n",
            "Epoch 69/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.3895 - accuracy: 0.8111\n",
            "Epoch 70/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3884 - accuracy: 0.8094\n",
            "Epoch 71/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3887 - accuracy: 0.8111\n",
            "Epoch 72/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3868 - accuracy: 0.8127\n",
            "Epoch 73/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3871 - accuracy: 0.8094\n",
            "Epoch 74/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3867 - accuracy: 0.8143\n",
            "Epoch 75/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3856 - accuracy: 0.8143\n",
            "Epoch 76/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.3846 - accuracy: 0.8160\n",
            "Epoch 77/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3855 - accuracy: 0.8111\n",
            "Epoch 78/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3850 - accuracy: 0.8127\n",
            "Epoch 79/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3835 - accuracy: 0.8208\n",
            "Epoch 80/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3826 - accuracy: 0.8127\n",
            "Epoch 81/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3836 - accuracy: 0.8176\n",
            "Epoch 82/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3829 - accuracy: 0.8176\n",
            "Epoch 83/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3819 - accuracy: 0.8094\n",
            "Epoch 84/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.3812 - accuracy: 0.8176\n",
            "Epoch 85/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3804 - accuracy: 0.8160\n",
            "Epoch 86/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3802 - accuracy: 0.8111\n",
            "Epoch 87/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3804 - accuracy: 0.8192\n",
            "Epoch 88/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3794 - accuracy: 0.8160\n",
            "Epoch 89/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3792 - accuracy: 0.8192\n",
            "Epoch 90/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3789 - accuracy: 0.8192\n",
            "Epoch 91/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.3785 - accuracy: 0.8160\n",
            "Epoch 92/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3790 - accuracy: 0.8160\n",
            "Epoch 93/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3790 - accuracy: 0.8176\n",
            "Epoch 94/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3779 - accuracy: 0.8208\n",
            "Epoch 95/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3764 - accuracy: 0.8192\n",
            "Epoch 96/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3762 - accuracy: 0.8143\n",
            "Epoch 97/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3767 - accuracy: 0.8192\n",
            "Epoch 98/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3765 - accuracy: 0.8143\n",
            "Epoch 99/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3755 - accuracy: 0.8127\n",
            "Epoch 100/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3747 - accuracy: 0.8160\n",
            "Epoch 101/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3737 - accuracy: 0.8160\n",
            "Epoch 102/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3736 - accuracy: 0.8160\n",
            "Epoch 103/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3725 - accuracy: 0.8192\n",
            "Epoch 104/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3722 - accuracy: 0.8192\n",
            "Epoch 105/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3721 - accuracy: 0.8160\n",
            "Epoch 106/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3714 - accuracy: 0.8176\n",
            "Epoch 107/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3706 - accuracy: 0.8160\n",
            "Epoch 108/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3703 - accuracy: 0.8192\n",
            "Epoch 109/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3701 - accuracy: 0.8257\n",
            "Epoch 110/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3699 - accuracy: 0.8143\n",
            "Epoch 111/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.3697 - accuracy: 0.8176\n",
            "Epoch 112/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.3691 - accuracy: 0.8176\n",
            "Epoch 113/150\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 0.3681 - accuracy: 0.8208\n",
            "Epoch 114/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.3671 - accuracy: 0.8192\n",
            "Epoch 115/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.3679 - accuracy: 0.8208\n",
            "Epoch 116/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.3657 - accuracy: 0.8208\n",
            "Epoch 117/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.3650 - accuracy: 0.8208\n",
            "Epoch 118/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.3653 - accuracy: 0.8241\n",
            "Epoch 119/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.3642 - accuracy: 0.8208\n",
            "Epoch 120/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.3634 - accuracy: 0.8257\n",
            "Epoch 121/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.3630 - accuracy: 0.8290\n",
            "Epoch 122/150\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 0.3627 - accuracy: 0.8225\n",
            "Epoch 123/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3621 - accuracy: 0.8225\n",
            "Epoch 124/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.3615 - accuracy: 0.8241\n",
            "Epoch 125/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.3603 - accuracy: 0.8241\n",
            "Epoch 126/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.3605 - accuracy: 0.8274\n",
            "Epoch 127/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.3588 - accuracy: 0.8257\n",
            "Epoch 128/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.3599 - accuracy: 0.8257\n",
            "Epoch 129/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.3581 - accuracy: 0.8208\n",
            "Epoch 130/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.3571 - accuracy: 0.8274\n",
            "Epoch 131/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.3561 - accuracy: 0.8257\n",
            "Epoch 132/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.3558 - accuracy: 0.8290\n",
            "Epoch 133/150\n",
            "62/62 [==============================] - 0s 4ms/step - loss: 0.3550 - accuracy: 0.8257\n",
            "Epoch 134/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.3552 - accuracy: 0.8290\n",
            "Epoch 135/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.3540 - accuracy: 0.8274\n",
            "Epoch 136/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.3544 - accuracy: 0.8274\n",
            "Epoch 137/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3526 - accuracy: 0.8257\n",
            "Epoch 138/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3536 - accuracy: 0.8306\n",
            "Epoch 139/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3522 - accuracy: 0.8274\n",
            "Epoch 140/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3516 - accuracy: 0.8274\n",
            "Epoch 141/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3511 - accuracy: 0.8339\n",
            "Epoch 142/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3523 - accuracy: 0.8290\n",
            "Epoch 143/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3527 - accuracy: 0.8290\n",
            "Epoch 144/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3491 - accuracy: 0.8274\n",
            "Epoch 145/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3491 - accuracy: 0.8306\n",
            "Epoch 146/150\n",
            "62/62 [==============================] - 0s 3ms/step - loss: 0.3477 - accuracy: 0.8274\n",
            "Epoch 147/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3480 - accuracy: 0.8339\n",
            "Epoch 148/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3489 - accuracy: 0.8322\n",
            "Epoch 149/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3481 - accuracy: 0.8306\n",
            "Epoch 150/150\n",
            "62/62 [==============================] - 0s 2ms/step - loss: 0.3470 - accuracy: 0.8420\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f7c6ea60e20>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_, accuracy = model.evaluate(X_train, y_train)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFowTWT19eqS",
        "outputId": "9bb88a26-c371-436b-be3e-6892ac617588"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20/20 [==============================] - 0s 2ms/step - loss: 0.3427 - accuracy: 0.8388\n",
            "Accuracy: 0.8387622237205505\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El accuracy del conjunto de entrenamiento es de un 86,15%."
      ],
      "metadata": {
        "id": "FeFmnPLB9hLk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test)\n",
        "y_pred[:15]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dF8_GjjG9h-v",
        "outputId": "d96e3385-271a-4337-92dd-ed4549fba876"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 0s 2ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.3158147 ],\n",
              "       [0.0445143 ],\n",
              "       [0.05449964],\n",
              "       [0.22216803],\n",
              "       [0.351036  ],\n",
              "       [0.28264105],\n",
              "       [0.00152145],\n",
              "       [0.71034014],\n",
              "       [0.7688697 ],\n",
              "       [0.54263777],\n",
              "       [0.04980477],\n",
              "       [0.5990942 ],\n",
              "       [0.41251   ],\n",
              "       [0.56798035],\n",
              "       [0.02570826]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como vemos, el modelo no devuelve las clases 0 y 1 directamente, sino que requiere de un preprocesamiento previo:"
      ],
      "metadata": {
        "id": "HQ_t4_QM9sBl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_round = [round(x[0]) for x in y_pred]\n",
        "y_pred_round[:15]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFrD3OZ99sV9",
        "outputId": "37b91192-a5c1-41b3-b765-1aa5c4b0dd1b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(y_test, y_pred_round)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2DNZm_f98t-",
        "outputId": "b854c792-d973-44a8-c2d8-f91667e8a207"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7532467532467533"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejemplo 3: Clasificar imágenes"
      ],
      "metadata": {
        "id": "vo5R0bOo-PmA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MNIST es un conjunto de datos de imágenes de dígitos escritos a mano, desde 0 hasta 9."
      ],
      "metadata": {
        "id": "4SiIhlmh-U40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "metadata": {
        "id": "lvcKjDYw-UWd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5519cf53-f513-42c3-ad47-e13e790c892b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Seleccionar una imagen en particular, por ejemplo, la imagen con índice 0\n",
        "image_index = 0\n",
        "image = X_train[image_index]\n",
        "label = y_train[image_index]\n",
        "\n",
        "# Mostrar la imagen\n",
        "plt.imshow(image, cmap='gray')\n",
        "plt.title(f'Label: {label}')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "1XVih9G4_rBN",
        "outputId": "fdb2e54c-2e14-47b6-daa5-93535e058cee"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Los valores de los píxeles de las imágenes se normalizan para que estén en el rango de 0 a 1 en lugar de 0 a 255."
      ],
      "metadata": {
        "id": "jC0oZzYV-f7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizar los datos (transformamos los valores de los píxeles de 0-255 a 0-1)\n",
        "X_train, X_test = X_train / 255.0, X_test / 255.0"
      ],
      "metadata": {
        "id": "icJcpF96-buV"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se define la arquitectura de la red neuronal. En este caso, estamos utilizando un modelo secuencial simple con una capa de aplanamiento que transforma las imágenes 2D en vectores 1D, una capa densa con 128 neuronas y una capa de salida con 10 neuronas."
      ],
      "metadata": {
        "id": "YrklXMHE-kgM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "  # Capa que aplana la imagen de entrada de 28x28 píxeles a un vector de 784 elementos\n",
        "  tf.keras.layers.Flatten(input_shape = (28, 28)),\n",
        "  # Capa oculta densa con 128 neuronas y función de activación ReLU\n",
        "  tf.keras.layers.Dense(128, activation = \"relu\"),\n",
        "  # Capa de salida con 10 neuronas (una para cada dígito del 0 al 9)\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "GonMDwKh-krX"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "También añadimos el compilador de la red para definir el optimizador y la función de pérdida, como hicimos anteriormente:"
      ],
      "metadata": {
        "id": "sbjHuIXI-t_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = \"adam\", loss = SparseCategoricalCrossentropy(from_logits = True), metrics = [\"accuracy\"])"
      ],
      "metadata": {
        "id": "9bzAR8pg-uJf"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se entrena el modelo en el conjunto de entrenamiento durante un cierto número de épocas. Cuando se trabaja con imágenes es menos común utilizar el parámetro del batch_size:"
      ],
      "metadata": {
        "id": "aR5tLgfo--kF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train, epochs = 5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0ld8dAG--tp",
        "outputId": "2f55fa0b-7e23-455f-e907-98c169f32b53"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/backend.py:5727: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.2556 - accuracy: 0.9273\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1127 - accuracy: 0.9665\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0777 - accuracy: 0.9765\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0589 - accuracy: 0.9818\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.0448 - accuracy: 0.9863\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f7c6e8cd0c0>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_, accuracy = model.evaluate(X_train, y_train)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u795jS09_BZ-",
        "outputId": "a19bcbf7-4810-4559-8515-74898b4561da"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0282 - accuracy: 0.9919\n",
            "Accuracy: 0.9918666481971741\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(X_test,  y_test, verbose=2)\n",
        "\n",
        "print('\\nTest accuracy:', test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTPgOziA_DTX",
        "outputId": "d0e7bd38-42a0-4b27-f026-ec897423ce6b"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 - 1s - loss: 0.0741 - accuracy: 0.9782 - 563ms/epoch - 2ms/step\n",
            "\n",
            "Test accuracy: 0.9782000184059143\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Seleccionar una imagen del set de prueba\n",
        "image_index = 20\n",
        "image = X_test[image_index]\n",
        "true_label = y_test[image_index]\n",
        "\n",
        "# Hacer la predicción\n",
        "prediction = model.predict(image.reshape(1, 28, 28))\n",
        "predicted_label = prediction.argmax()\n",
        "\n",
        "# Mostrar la imagen junto con la etiqueta real y la etiqueta predicha\n",
        "plt.imshow(image, cmap='gray')\n",
        "plt.title(f'Real: {true_label}, Predicción: {predicted_label}')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "rBrSVrVIACL9",
        "outputId": "06df8887-5f14-48a9-8950-5400d999a615"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 52ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Links de interés <a name=\"links\"></a>"
      ],
      "metadata": {
        "id": "4ZLwGjES0O50"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- [Deep Learning by Ian Goodfellow](https://www.amazon.com/Deep-Learning-Adaptive-Computation-Machine/dp/0262035618/ref=sr_1_1?adgrpid=84169167569&dib=eyJ2IjoiMSJ9.XlOY_4EzkkbmP1JnvaZd2uSLI2n9vK_J2qknENcobM9t5wg5TDCDBRfMMlZyVY-e5eotgivbeK5QPIhQRZI4TZnEmB3k2xbgiR9sgSoXG2SIBW82_zcBg4K63e865-GYeq9ArgEE4QI26CfQmdS2kW10vny6cPgJJhaF5k1SVI17sPRfbeUDPyFwCZyP_VMqcSuswAcpwWAZPJlqGHYXIhUtqtY2uSH2aMkUCHKIq8Y.wzji_Zr_Q7bq4qvKU6YEHBVepSqmbgLrKLLk0gbQrTI&dib_tag=se&hvadid=585479342144&hvdev=c&hvlocphy=9041004&hvnetw=g&hvqmt=e&hvrand=1587894621549095832&hvtargid=kwd-298993905496&hydadcr=22371_13333115&keywords=deep+learning+ian+goodfellow&qid=1721946760&sr=8-1)\n",
        "- [Deep Learning with Python by Francois Chollet](https://www.amazon.com/Learning-Python-Second-Fran%C3%A7ois-Chollet/dp/1617296864/ref=sr_1_2_sspa?adgrpid=84169167569&dib=eyJ2IjoiMSJ9.XlOY_4EzkkbmP1JnvaZd2uSLI2n9vK_J2qknENcobM9t5wg5TDCDBRfMMlZyVY-e5eotgivbeK5QPIhQRZI4TZnEmB3k2xbgiR9sgSoXG2SIBW82_zcBg4K63e865-GYeq9ArgEE4QI26CfQmdS2kW10vny6cPgJJhaF5k1SVI17sPRfbeUDPyFwCZyP_VMqcSuswAcpwWAZPJlqGHYXIhUtqtY2uSH2aMkUCHKIq8Y.wzji_Zr_Q7bq4qvKU6YEHBVepSqmbgLrKLLk0gbQrTI&dib_tag=se&hvadid=585479342144&hvdev=c&hvlocphy=9041004&hvnetw=g&hvqmt=e&hvrand=1587894621549095832&hvtargid=kwd-298993905496&hydadcr=22371_13333115&keywords=deep+learning+ian+goodfellow&qid=1721946760&sr=8-2-spons&sp_csd=d2lkZ2V0TmFtZT1zcF9hdGY&psc=1)\n",
        "- [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/)"
      ],
      "metadata": {
        "id": "R75ytPht0dY7"
      }
    }
  ]
}